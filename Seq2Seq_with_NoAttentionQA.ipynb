{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ambideXtrous9/Seq2Seq-Attention-QA/blob/main/Seq2Seq_with_NoAttentionQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffw5Y9UlsSy6",
        "outputId": "41a084f5-bc7a-44e1-8005-b7f891b74735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.0/719.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet transformers\n",
        "!pip install --quiet pytorch-lightning\n",
        "!pip install --quiet tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qw31Zi_sr-WZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.set_float32_matmul_precision('high')\n",
        "import random\n",
        "import numpy as np\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_lightning import Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer as Tokenizer,AutoModelForSeq2SeqLM\n",
        "from torch.optim import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnr6M55stxqS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKKJM61osa-o"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'google/flan-t5-small'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQxweNMf-Gpe"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3QaqBEitu1x"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/MTP CODE/NewsQA_SPAN.feather'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq8Zycmgtzpt"
      },
      "outputs": [],
      "source": [
        "df = pd.read_feather(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAyD5DlMuAqM"
      },
      "outputs": [],
      "source": [
        "df = df.iloc[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQFNNpzvt_CB"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(df,test_size=0.2)\n",
        "val_df, test_df = train_test_split(val_df,test_size=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRtAcjRiryT4"
      },
      "outputs": [],
      "source": [
        "class QADataset(Dataset):\n",
        "  def __init__(self,data : pd.DataFrame,tokenizer : Tokenizer,source_max_token_len : int = 200,target_max_token_len : int = 15):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = data\n",
        "    self.source_max_token_len = source_max_token_len\n",
        "    self.target_max_token_len = target_max_token_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self,index : int):\n",
        "    data_row = self.data.iloc[index]\n",
        "\n",
        "    source_encoding = tokenizer(\n",
        "        data_row['question'],\n",
        "        data_row['paragraph'],\n",
        "        max_length = self.source_max_token_len,\n",
        "        padding = \"max_length\",\n",
        "        truncation = \"only_second\",\n",
        "        return_attention_mask = True,\n",
        "        add_special_tokens = True,\n",
        "        return_tensors = \"pt\")\n",
        "    \n",
        "    target_encoding = tokenizer(\n",
        "        data_row['answer'],\n",
        "        max_length = self.target_max_token_len,\n",
        "        padding = \"max_length\",\n",
        "        truncation = True,\n",
        "        return_attention_mask = True,\n",
        "        add_special_tokens = True,\n",
        "        return_tensors = \"pt\")\n",
        "    \n",
        "    labels = target_encoding[\"input_ids\"]\n",
        "    #labels[labels == 0] = -100\n",
        "\n",
        "    return dict(\n",
        "        answer = data_row['answer'],\n",
        "        input_ids = source_encoding['input_ids'].flatten(),\n",
        "        attention_mask = source_encoding['attention_mask'].flatten(),\n",
        "        labels = labels.flatten())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRagDS1h9-JJ"
      },
      "outputs": [],
      "source": [
        "sample_dataset = QADataset(df,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7o0a9qP-tKd"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp1VtEw39-3j"
      },
      "outputs": [],
      "source": [
        "for data in sample_dataset:\n",
        "  print(type(data['input_ids']))\n",
        "  print(data['input_ids'])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shyhmD2rsJWY"
      },
      "outputs": [],
      "source": [
        "class QADataModule(pl.LightningDataModule):\n",
        "  def __init__(self,train_df , val_df, test_df,tokenizer : Tokenizer,batch_size : int = 8,source_max_token_len : int = 200,target_max_token_len : int = 15):\n",
        "    super().__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.train_df = train_df\n",
        "    self.test_df = test_df\n",
        "    self.val_df = val_df\n",
        "    self.tokenizer = tokenizer\n",
        "    self.source_max_token_len = source_max_token_len\n",
        "    self.target_max_token_len = target_max_token_len\n",
        "\n",
        "  def setup(self,stage=None):\n",
        "    self.train_dataset = QADataset(self.train_df,self.tokenizer,self.source_max_token_len,self.target_max_token_len)\n",
        "    self.val_dataset = QADataset(self.val_df,self.tokenizer,self.source_max_token_len,self.target_max_token_len)\n",
        "    self.test_dataset = QADataset(self.test_df,self.tokenizer,self.source_max_token_len,self.target_max_token_len)\n",
        "    \n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.train_dataset,batch_size = self.batch_size,shuffle=True,num_workers=2)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.val_dataset,batch_size = self.batch_size,num_workers=2)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.test_dataset,batch_size = self.batch_size,num_workers=2)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuq2U465y5Li"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj0Y8Mtqr1Q0"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        outputs, hidden = self.gru(embedded)\n",
        "        return outputs, hidden\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDNcHWwpHKeq"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        max_len = encoder_outputs.size(1)\n",
        "        h = hidden.repeat(max_len, 1, 1).transpose(0, 1)\n",
        "        encoder_outputs = encoder_outputs.transpose(1, 2)\n",
        "        hidden_linear = hidden[-1].unsqueeze(0).unsqueeze(2)  # add extra dim for bmm\n",
        "        hidden_linear = self.attn(hidden_linear.transpose(1, 2)).transpose(1, 2)  # transpose for correct shape\n",
        "        attn_energies = self.v(torch.tanh(hidden_linear + encoder_outputs))\n",
        "        attn_weights = F.softmax(attn_energies, dim=1)\n",
        "        context = torch.bmm(attn_weights.transpose(1,2), encoder_outputs)\n",
        "        return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LLBr59bsF8q"
      },
      "outputs": [],
      "source": [
        "class AttentionDecoder(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, num_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
        "        self.attention = Attention(hidden_size)\n",
        "\n",
        "    def forward(self, x, hidden, encoder_outputs):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        context, _ = self.attention(hidden, encoder_outputs)\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.gru(rnn_input, hidden)\n",
        "        output = self.fc_out(output)\n",
        "        return output, hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-wgRip9sIqd"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size, num_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
        "        self.attention = Attention(hidden_size)\n",
        "\n",
        "    def forward(self, x, hidden, encoder_outputs):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        #context = self.attention(hidden[-1], encoder_outputs)\n",
        "        #rnn_input = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.fc_out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrymTLO3no1b"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(pl.LightningModule):\n",
        "    def __init__(self, encoder, decoder, pad_idx):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.trained_model = NQAModel.load_from_checkpoint(cppath)\n",
        "        self.trained_model.freeze()\n",
        "\n",
        "    def forward(self, src, attn,trg):\n",
        "        batch_size = src.size(0)\n",
        "        max_len = trg.size(1)\n",
        "        trg_vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "\n",
        "        outputs = torch.zeros(batch_size, max_len, trg_vocab_size).to(self.device)\n",
        "        output = trg[:, 0]\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden = self.decoder(output.unsqueeze(1), hidden, encoder_outputs)\n",
        "            outputs[:, t, :] = output.squeeze(1)\n",
        "            top1 = output.argmax(2)\n",
        "            output = top1.squeeze(1)\n",
        "        \n",
        "        return outputs, hidden\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \n",
        "        src = batch['input_ids']\n",
        "        attn = batch['attention_mask']\n",
        "        trg = batch['labels']\n",
        "\n",
        "        trg_input = trg\n",
        "        trg_output = trg\n",
        "\n",
        "        output, hidden = self(src, attn,trg_input)\n",
        "\n",
        "\n",
        "        output = output.reshape(-1, output.shape[-1])\n",
        "        trg_output = trg_output.reshape(-1)\n",
        "\n",
        "        train_loss = F.cross_entropy(output, trg_output, ignore_index=self.pad_idx)\n",
        "\n",
        "        self.log_dict({\"train_loss\" : train_loss,\n",
        "                       },prog_bar=True,logger=True)\n",
        "\n",
        "        return train_loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        src = batch['input_ids']\n",
        "        attn = batch['attention_mask']\n",
        "        trg = batch['labels']\n",
        "\n",
        "        trg_input = trg\n",
        "        trg_output = trg\n",
        "\n",
        "        output, hidden = self(src, attn,trg_input)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        output = output.reshape(-1, output.shape[-1])\n",
        "        trg_output = trg_output.reshape(-1)\n",
        "\n",
        "        val_loss = F.cross_entropy(output, trg_output, ignore_index=self.pad_idx)\n",
        "\n",
        "        self.log_dict({\"val_loss\" : val_loss\n",
        "                       },prog_bar=True,logger=True)\n",
        "\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
        "        return optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZMwvSKVyI1m"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "N_EPOCHS = 50\n",
        "\n",
        "data_module = QADataModule(train_df,val_df,test_df,tokenizer,batch_size = BATCH_SIZE)\n",
        "data_module.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BtNF4TFvYJX"
      },
      "outputs": [],
      "source": [
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath = 'checkpoints',\n",
        "    filename = 'Seq2Seq',\n",
        "    save_top_k = 1,\n",
        "    verbose = True,\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'min'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkWYxy0L8gEh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RtNw1sbqMDv"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(input_size=tokenizer.vocab_size, hidden_size=512, num_layers=1, dropout=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_fSWTBUs9Ko"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(output_size=tokenizer.vocab_size, hidden_size=512, num_layers=1, dropout=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkCFZjFQvTlU"
      },
      "outputs": [],
      "source": [
        "model = Seq2Seq(encoder,decoder,pad_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWK8ZmAcvipV"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(devices=-1, accelerator=\"gpu\",\n",
        "    callbacks=[checkpoint_callback],\n",
        "    max_epochs = N_EPOCHS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVaVhdtLsMZB"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model,data_module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdg-I8yUEL7e"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ANDL6KPQlCK"
      },
      "outputs": [],
      "source": [
        "def predict(question):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the source text\n",
        "    source_tokens = tokenizer(\n",
        "        question['question'],\n",
        "        question['paragraph'],\n",
        "        max_length=200,\n",
        "        padding=\"max_length\",\n",
        "        truncation=\"only_second\",\n",
        "        add_special_tokens=True,\n",
        "        return_tensors=\"pt\")['input_ids'].flatten().to(device)\n",
        "\n",
        "    # Reshape the source tokens to match the expected input shape of the encoder\n",
        "    source_tokens = source_tokens.unsqueeze(0).to(device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Encode the source text\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(source_tokens)\n",
        "\n",
        "    max_length = 15\n",
        "    # Initialize the predicted sentence\n",
        "    outputs = [0]\n",
        "\n",
        "    # Generate the output sequence token by token\n",
        "    for _ in range(max_length):\n",
        "        previous_word = torch.LongTensor([outputs[-1]]).unsqueeze(0).to(device)\n",
        "\n",
        "        # Decode the next token\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(previous_word, hidden, encoder_outputs)\n",
        "            best_guess = output.argmax(2).item()\n",
        "\n",
        "        # Add the predicted token to the predicted sentence\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # If the predicted token is the end-of-sequence token, stop generating further tokens\n",
        "        if best_guess == tokenizer.sep_token_id:\n",
        "            break\n",
        "\n",
        "    # Convert the predicted sentence back to text\n",
        "    \n",
        "    print(outputs)\n",
        "    predicted_text = tokenizer.decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    return predicted_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWcQ4GzBFhPp"
      },
      "outputs": [],
      "source": [
        "sample_question = test_df.iloc[10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj9dKEfdGOhs"
      },
      "outputs": [],
      "source": [
        "sample_question['question']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHuEJ-s5GW7V"
      },
      "outputs": [],
      "source": [
        "sample_question['answer']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttsfTDR9Fs1l"
      },
      "outputs": [],
      "source": [
        "predict(sample_question)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}